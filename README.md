## Unsupervised Hallucincation Detection by Inspecting Reasoning Processes

This repository contains the official implementation of  **EMNLP 2025 main paper** ["Unsupervised Hallucincation Detection by Inspecting Reasoning Processes"](https://arxiv.org/abs/2509.10004). We propose Internal Reasoning for Inference of Statement veracity (IRIS), an unsupervised hallucination detection framework inspects the uncertainty of the reasoning process when determining the factuality of a statement.
 
The detection pipeline works in two stages:
1. For each statement in the dataset, we prompt a proxy model to *carefully verify its truth value*. We obtain (i). the contextualized embeddings as features; and (ii). the uncertainty in its response as a pseudolabel.
2. We train a binary classifier to determine whether a statement is hallucinated using the model activations as features, along with the uncertainty pseudolabels.

<p align="center">
 <img height="280" alt="Image" src="https://github.com/user-attachments/assets/a9c8062e-d536-4b8e-905a-d0c15bb8f004" />
</p> 

IRIS is fully unsupervised, computationally low cost, and works well even with few training data, making it suitable for real-time detection.

### Get Started

Create a conda environment and install requirements.

```
conda create -n iris python=3.12
conda activate iris
pip install -r requirements.txt
```

Generate embeddings.
```
python generate_embeddings.py
```

Then, train classifier. Hyperparameters may need finetuning.
```
python train_classifier.py
```

The script `scripts/run.sh` can be used to reproduce experiments in the paper.

### Citation

If you find this work useful, please cite:
```
@inproceedings{srey-etal-2025-unsupervised,
    title = "Unsupervised Hallucination Detection by Inspecting Reasoning Processes",
    author = "Srey, Ponhvoan  and
      Wu, Xiaobao  and
      Luu, Anh Tuan",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.1124/",
    doi = "10.18653/v1/2025.emnlp-main.1124",
    pages = "22128--22140",
    ISBN = "979-8-89176-332-6",
    abstract = "Unsupervised hallucination detection aims to identify hallucinated content generated by large language models (LLMs) without relying on labeled data. While unsupervised methods have gained popularity by eliminating labor-intensive human annotations, they frequently rely on proxy signals unrelated to factual correctness. This misalignment biases detection probes toward superficial or non-truth-related aspects, limiting generalizability across datasets and scenarios. To overcome these limitations, we propose IRIS, an unsupervised hallucination detection framework, leveraging internal representations intrinsic to factual correctness. IRIS prompts the LLM to carefully verify the truthfulness of a given statement, and obtain its contextualized embedding as informative features for training. Meanwhile, the uncertainty of each response is considered a soft pseudolabel for truthfulness. Experimental results demonstrate that IRIS consistently outperforms existing unsupervised methods. Our approach is fully unsupervised, computationally low cost, and works well even with few training data, making it suitable for real-time detection."
}
```
